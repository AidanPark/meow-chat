from __future__ import annotations

import asyncio
from threading import Thread, Event, current_thread
from queue import Queue, Empty
import time
from typing import Dict, Any, Generator, List, Set
import logging
import traceback
import os
import asyncio

from langchain_core.callbacks import BaseCallbackHandler
from services.orchestrator import (
    react_plan_node,
    react_execute_node,
    react_self_eval_node,
    react_compose_node,
    OrchestratorState,
)

# ì´ ëª¨ë“ˆì€ LangGraph ê¸°ë°˜ ì—ì´ì „íŠ¸ë¥¼ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰í•˜ê³ ,
# ì–»ì–´ì§„ í† í° ìŠ¤íŠ¸ë¦¼ì„ Streamlit UIì— ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•œë‹¤.
# - UIStreamingCallbackHandler: LangChain ì½œë°±ì—ì„œ í† í°/ë„êµ¬ ì‚¬ìš© ì •ë³´ë¥¼ ìˆ˜ì§‘
# - stream_agent_generator: ë¹„ë™ê¸°ë¡œ ì—ì´ì „íŠ¸ë¥¼ ëŒë¦¬ê³ , íì— ìŒ“ì¸ í† í°ì„ ì œë„ˆë ˆì´í„°ë¡œ ë°©ì¶œ
# rec ë”•ì…”ë„ˆë¦¬ì—ëŠ” ìµœì¢… ì‘ë‹µ í…ìŠ¤íŠ¸, í† í° ëª©ë¡, ì‚¬ìš©ëœ ë„êµ¬ ë‚´ì—­ ë“±ì´ ì±„ì›Œì ¸
# í˜¸ì¶œ ì¸¡(frontend/app.py)ì—ì„œ UI ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë° í™œìš©ëœë‹¤.


# ------------------------------------------------------------
# Logging setup (console + file) for streaming exceptions
# ------------------------------------------------------------
_LOGGER = logging.getLogger("meow.frontend.streaming")
if not _LOGGER.handlers:
    _LOGGER.setLevel(logging.INFO)
    # Console handler
    _ch = logging.StreamHandler()
    _ch.setLevel(logging.INFO)
    _fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")
    _ch.setFormatter(_fmt)
    _LOGGER.addHandler(_ch)

    # File handler under frontend/logs/
    try:
        _BASE_DIR = os.path.dirname(os.path.dirname(__file__))  # frontend/
        _LOG_DIR = os.path.join(_BASE_DIR, "logs")
        os.makedirs(_LOG_DIR, exist_ok=True)
        _fh = logging.FileHandler(os.path.join(_LOG_DIR, "streaming.log"))
        _fh.setLevel(logging.INFO)
        _fh.setFormatter(_fmt)
        _LOGGER.addHandler(_fh)
    except Exception:
        # If file handler fails (permissions, etc.), continue with console-only
        pass


def _format_exception_recursive(exc: BaseException) -> str:
    """Format an exception, expanding ExceptionGroup recursively to capture sub-exceptions."""
    lines: List[str] = []
    try:
        lines.append("".join(traceback.format_exception(type(exc), exc, exc.__traceback__)))
        # Python 3.11 ExceptionGroup compatibility
        subs = getattr(exc, "exceptions", None)
        if isinstance(subs, (list, tuple)) and subs:
            for idx, sub in enumerate(subs):
                lines.append(f"\n-- Sub-exception #{idx+1} ------------------------------------------------\n")
                try:
                    lines.append("".join(traceback.format_exception(type(sub), sub, sub.__traceback__)))
                except Exception:
                    lines.append(repr(sub))
    except Exception:
        try:
            lines.append(repr(exc))
        except Exception:
            pass
    return "".join(lines)


class UIStreamingCallbackHandler(BaseCallbackHandler):
    """LLM í† í°ê³¼ ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ íì— í˜ë ¤ë³´ë‚´ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì½œë°±."""

    def __init__(self, token_queue: Queue, done_event: Event, rec: Dict[str, Any]):
        # recì—ëŠ” ì‹¤í–‰ ì¤‘ ìˆ˜ì§‘í•œ í† í°, ì‚¬ìš©ëœ ë„êµ¬, ë„êµ¬ ì¸ì, ìµœì¢… ì‘ë‹µ ë“±ì„ ëˆ„ì í•œë‹¤.
        self.token_queue = token_queue
        self.done_event = done_event
        self.rec = rec

    def on_llm_new_token(self, token: str, **kwargs):  # type: ignore[override]
        # ìƒˆ í† í°ì´ ë“¤ì–´ì˜¤ë©´ ì¦‰ì‹œ í† í° íì™€ ê¸°ë¡ì— ì¶”ê°€í•œë‹¤.
        try:
            if token:
                self.rec.setdefault("tokens", []).append(token)
                self.token_queue.put(token)
        except Exception:
            pass

    def on_tool_start(self, serialized=None, input_str=None, **kwargs):  # type: ignore[override]
        # LangGraph ì—ì´ì „íŠ¸ê°€ ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ê¸° ì§ì „ì— ë„êµ¬ ì´ë¦„ê³¼ ì¸ìë¥¼ ê¸°ë¡í•œë‹¤.
        try:
            name = None
            if isinstance(serialized, dict):
                name = serialized.get("name") or serialized.get("id")
            if name:
                used: Set[str] = self.rec.setdefault("used_tools", set())
                used.add(name)
                details: List[Dict[str, Any]] = self.rec.setdefault("tool_details", [])
                details.append({"name": name, "args": input_str})
        except Exception:
            pass


def stream_agent_generator(
    lc_messages: List[Dict[str, Any]],
    rec: Dict[str, Any],
    model,
    client,
) -> Generator[str, None, None]:
    """LangGraph ì—ì´ì „íŠ¸ë¥¼ ë¹„ë™ê¸° ì‹¤í–‰í•˜ê³  í† í°ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‚´ë³´ë‚¸ë‹¤."""
    token_queue: Queue[str] = Queue()
    done = Event()

    handler = UIStreamingCallbackHandler(token_queue, done, rec)

    async def _run_async():
        try:
            # ì§€ì—° ì„í¬íŠ¸: í™˜ê²½ë³„ í˜¸í™˜ì„± ë³´ì •ì„ ìœ„í•´ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì„í¬íŠ¸
            from services.langgraph_compat import create_react_agent_executor  # type: ignore
            tools = await client.get_tools()
            agent = create_react_agent_executor(model, tools)
            result = await agent.ainvoke({"messages": lc_messages}, config={"callbacks": [handler]})

            messages = None
            if isinstance(result, dict):
                messages = result.get("messages") or result.get("outputs")
            elif isinstance(result, list):
                messages = result

            if messages:
                final = messages[-1]
                rec["final_text"] = getattr(final, "content", None) or str(final)
        except Exception as e:
            rec["error"] = str(e)
            token_queue.put(f"\n[ì—ëŸ¬] {e}")
        finally:
            done.set()

    thread = Thread(target=lambda: asyncio.run(_run_async()), daemon=True)
    thread.start()

    aggregated: List[str] = []
    while not (done.is_set() and token_queue.empty()):
        try:
            token = token_queue.get(timeout=0.05)
            aggregated.append(token)
            yield token
        except Empty:
            time.sleep(0.01)

    if not aggregated:
        final_text = rec.get("final_text")
        if final_text:
            yield final_text


def stream_react_rag_generator(
    user_request: str,
    rec: Dict[str, Any],
    model,
    client,
    allowed_tools: List[str] | None = None,
    max_iters: int = 4,
    extra_vars: Dict[str, Any] | None = None,
) -> Generator[str, None, None]:
    """ReAct ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¥¼ ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰í•˜ë©° ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ë¥¼ ë°©ì¶œí•©ë‹ˆë‹¤.

    - ê³„íš/ì‹¤í–‰/ìê°€í‰ê°€ ë£¨í”„ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ì§„í–‰ ìƒí™©ì„ ì¦‰ì‹œ ë³´ì—¬ì¤ë‹ˆë‹¤.
    - ìµœì¢… ë‹µë³€ì€ ëª¨ë¸ astreamìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì„œìˆ í•˜ë©° í† í° ìŠ¤íŠ¸ë¦¼ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
    - recì—ëŠ” tokens, used_tools, tool_details, final_textë¥¼ ëˆ„ì í•©ë‹ˆë‹¤.
    """
    token_queue: Queue[str] = Queue()
    done = Event()

    def _qput(txt: str):
        try:
            if txt:
                token_queue.put(txt)
        except Exception:
            pass

    async def _run_async():
        try:
            # ì´ˆê¸° ìƒíƒœ êµ¬ì„±
            from typing import cast
            state: OrchestratorState = cast(OrchestratorState, {
                "user_request": user_request,
                "allowed_tools": allowed_tools or [],
                "vars": dict(extra_vars or {}),
                "outputs": {},
                "errors": [],
                "warnings": [],
                "tools_used": [],
                "react_iteration": 0,
                "react_max_iters": int(max_iters),
                "react_history": [],
            })

            # ë£¨í”„ ì‹¤í–‰: ê³„íš â†’ ì‹¤í–‰ â†’ ìê°€í‰ê°€
            for _ in range(int(max_iters)):
                state = await react_plan_node(state, model, client)
                finish_hint = state.get("react_finish")
                plan = state.get("plan") or {}
                steps = plan.get("steps") or []
                if steps:
                    step = steps[0]
                    tool = step.get("tool")
                    args = step.get("args") or {}
                    _qput(f"ê³„íš: {tool} ì‹¤í–‰ ì¤€ë¹„\n")
                if finish_hint:
                    break

                state = await react_execute_node(state, client)
                # ë§ˆì§€ë§‰ ê´€ì°° ìŠ¤íŠ¸ë¦¬ë°
                hist = state.get("react_history") or []
                if hist:
                    last = hist[-1]
                    step = last.get("step") or {}
                    tool = step.get("tool")
                    args = step.get("args") or {}
                    obs = last.get("observation")
                    # ì‚¬ìš©ëœ ë„êµ¬ ê¸°ë¡
                    try:
                        used: Set[str] = rec.setdefault("used_tools", set())
                        if tool:
                            used.add(str(tool))
                        details: List[Dict[str, Any]] = rec.setdefault("tool_details", [])
                        details.append({"name": tool, "args": args})
                    except Exception:
                        pass
                    if tool:
                        _qput(f"ğŸ› ï¸ {tool} í˜¸ì¶œ\n")
                    if obs is not None:
                        # ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½í•´ì„œ ì¶œë ¥
                        try:
                            txt = str(obs)
                            if len(txt) > 600:
                                txt = txt[:600] + "â€¦"
                            _qput(f"ê´€ì°°: {txt}\n")
                        except Exception:
                            pass

                state = await react_self_eval_node(state, model)
                cont = bool(state.get("react_should_continue"))
                if cont:
                    _qput("ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤â€¦\n")
                    continue
                break

            # ìµœì¢… ë©”ì‹œì§€ í•©ì„±
            state = react_compose_node(state)
            final_msg = state.get("message") or ""
            outs = state.get("outputs") or {}
            errs = state.get("errors") or []

            # ìŠ¤ëª°í† í¬/ë¬´í”Œëœ í´ë°±: ëª¨ë¸ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì§ì ‘ ì‘ë‹µ ìƒì„±
            no_effective_plan = (not final_msg or str(final_msg).strip() in ("", "ì‹¤í–‰í•  ê³„íšì´ ì—†ì—ˆìŠµë‹ˆë‹¤.")) and (not outs) and (not errs)
            if no_effective_plan:
                sys_prompt = (
                    "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ê°„ê²°í•œ í•œêµ­ì–´ ë¹„ì„œì…ë‹ˆë‹¤. ì‘ì€ ì¸ì‚¬ë§ì´ë‚˜ ì¡ë‹´ì—ë„ ë”°ëœ»í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”.\n"
                    "ë¶ˆí•„ìš”í•œ ë„êµ¬ ì‚¬ìš© ê³„íšì€ ë§Œë“¤ì§€ ë§ê³ , ì§€ê¸ˆ ì…ë ¥ì— ì¹œê·¼í•˜ê²Œ ë‹µë³€ë§Œ ì¶œë ¥í•˜ì„¸ìš”."
                )
                vars_map = dict(extra_vars or {})
                pinned = vars_map.get("pinned_core_facts") if isinstance(vars_map, dict) else None
                if pinned:
                    sys_prompt += f"\n[í•µì‹¬ ì‚¬ì‹¤ ìš”ì•½]\n{str(pinned)[:800]}\n"
                async for chunk in model.astream([
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_request.strip()},
                ]):
                    # LangChainì˜ AIMessageChunk ë“±ì—ì„œ contentë§Œ ì¶”ì¶œ, ë©”íƒ€/ë¹ˆ ì²­í¬ëŠ” ë¬´ì‹œ
                    token = None
                    try:
                        token = getattr(chunk, "content", None)
                    except Exception:
                        token = None
                    if token:
                        rec.setdefault("tokens", []).append(token)
                        _qput(token)
                rec["final_text"] = "".join(rec.get("tokens", []))
                return

            # ì¼ë°˜ ì¼€ì´ìŠ¤: ê´€ì°°ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì •ë¦¬í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë°
            try:
                history = state.get("react_history") or []
                brief_obs: List[str] = []
                for h in history[-5:]:  # ìµœê·¼ 5ê°œë§Œ ìš”ì•½
                    step = (h or {}).get("step") or {}
                    tool = step.get("tool")
                    obs = (h or {}).get("observation")
                    line = f"- {tool}: {str(obs)[:200]}" if tool else f"- {str(obs)[:200]}"
                    brief_obs.append(line)

                sys_prompt = (
                    "ë‹¹ì‹ ì€ ì¹œì ˆí•œ í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì‚¬ìš©ì ìš”ì²­ê³¼ ë„êµ¬ ê´€ì°°ì„ ì°¸ê³ í•˜ì—¬ ê°„ê²°í•˜ê³  ë„ì›€ë˜ëŠ” ë‹µë³€ë§Œ ì¶œë ¥í•˜ì„¸ìš”."
                )
                user_blk = (
                    f"ìš”ì²­: {user_request}\n\n"
                    f"ê´€ì°° ìš”ì•½:\n" + ("\n".join(brief_obs) if brief_obs else "(ì—†ìŒ)") + "\n\n"
                    f"ì´ˆì•ˆ: {final_msg}\n"
                )
                tokens: List[str] = []
                async for chunk in model.astream([
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_blk},
                ]):
                    # contentë§Œ ì¶œë ¥í•˜ê³ , ë¹ˆ/ë©”íƒ€ ì²­í¬ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.
                    token = None
                    try:
                        token = getattr(chunk, "content", None)
                    except Exception:
                        token = None
                    if token:
                        tokens.append(token)
                        rec.setdefault("tokens", []).append(token)
                        _qput(token)
                rec["final_text"] = "".join(tokens) if tokens else str(final_msg)
            except Exception as e:
                # ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ìµœì¢… ë©”ì‹œì§€ë¥¼ í•œ ë²ˆì— ì¶œë ¥
                txt = str(final_msg)
                rec["final_text"] = txt
                _qput(txt)
                try:
                    _LOGGER.error(
                        "[STREAMING ERROR] compose streaming failure: %s",
                        str(e),
                    )
                except Exception:
                    pass
        except Exception as e:
            rec["error"] = str(e)
            # Structured logging: capture full stack including ExceptionGroup sub-exceptions
            try:
                loop_info = None
                try:
                    loop = asyncio.get_running_loop()
                    loop_info = f"loop_id={id(loop)}"
                except Exception:
                    loop_info = "loop_id=NA"
                ctx = {
                    "thread": current_thread().name,
                    "loop": loop_info,
                }
                _LOGGER.error(
                    "[STREAMING ERROR] ReAct astream failure | thread=%s | %s | request_head=%r\n%s",
                    ctx["thread"], ctx["loop"], (user_request[:200] if isinstance(user_request, str) else user_request), _format_exception_recursive(e)
                )
            except Exception:
                pass
            # Also surface a short marker to the UI stream so user sees something immediate
            _qput("\n[ì—ëŸ¬] streaming failure - see frontend/logs/streaming.log")
        finally:
            done.set()

    thread = Thread(target=lambda: asyncio.run(_run_async()), daemon=True)
    thread.start()

    aggregated: List[str] = []
    while not (done.is_set() and token_queue.empty()):
        try:
            token = token_queue.get(timeout=0.05)
            aggregated.append(token)
            yield token
        except Empty:
            time.sleep(0.01)

    if not aggregated:
        final_text = rec.get("final_text")
        if final_text:
            yield final_text
