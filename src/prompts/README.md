# Lab Extraction Prompts

ê²€ì‚¬ì§€ ì¶”ì¶œ ë° ì±—ë´‡ ì‘ë‹µì— ì‚¬ìš©ë˜ëŠ” LLM í”„ë¡¬í”„íŠ¸ë¥¼ ì¤‘ì•™ ê´€ë¦¬í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

## ğŸ“‚ êµ¬ì¡°

```
src/prompts/
â”œâ”€â”€ __init__.py                    # í”„ë¡¬í”„íŠ¸ export
â”œâ”€â”€ metadata_extraction.py         # patient_name ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
â”œâ”€â”€ header_inference.py            # í…Œì´ë¸” í—¤ë” ì¶”ë¡  í”„ë¡¬í”„íŠ¸
â”œâ”€â”€ chat.py                        # ìŠ¤ëª°í†¡/ì¼ë°˜ ëŒ€í™” í”„ë¡¬í”„íŠ¸
â”œâ”€â”€ lab_analysis.py                # ê²€ì‚¬ì§€ ë¶„ì„ í”„ë¡¬í”„íŠ¸
â”œâ”€â”€ intent_classification.py       # ì˜ë„ ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸
â””â”€â”€ README.md                      # ì‚¬ìš© ê°€ì´ë“œ
```

## ğŸ¯ ì„¤ê³„ ëª©ì 

### 1. **ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ**
- í”„ë¡¬í”„íŠ¸ë¥¼ ì½”ë“œì—ì„œ ë¶„ë¦¬
- í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ì‹œ Python ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”
- ë²„ì „ ê´€ë¦¬ ìš©ì´

### 2. **ì¬ì‚¬ìš©ì„±**
- ì—¬ëŸ¬ ê³³ì—ì„œ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ import
- ì¤‘ë³µ ì œê±°

### 3. **í˜‘ì—… í¸ì˜ì„±**
- ë¹„ê°œë°œìë„ í”„ë¡¬í”„íŠ¸ ê²€í† /ìˆ˜ì • ê°€ëŠ¥
- í”„ë¡¬í”„íŠ¸ë§Œ ëª¨ì•„ì„œ ë¦¬ë·° ê°€ëŠ¥

## ğŸ“ ì‚¬ìš©ë²•

### Import
```python
from src.prompts import (
    PATIENT_NAME_SYSTEM_PROMPT,
    HEADER_INFERENCE_SYSTEM_PROMPT,
)
from src.prompts.metadata_extraction import (
    format_patient_name_user_prompt
)
```

### ì‚¬ìš© ì˜ˆì‹œ

#### 1. Patient Name ì¶”ì¶œ
```python
# System prompt
system_prompt = PATIENT_NAME_SYSTEM_PROMPT

# User prompt í¬ë§·íŒ…
header_text = """
24ì‹œ í«í”ŒëŸ¬ìŠ¤ ë™ë¬¼ë³‘ì›
ë³´í˜¸ì: ê¹€ì² ìˆ˜
ë‚˜ë¹„
ê²€ì‚¬ì¼: 2025-01-20
"""

user_prompt = format_patient_name_user_prompt(
    header_text=header_text,
    client_name="ê¹€ì² ìˆ˜"  # client_nameê³¼ í˜¼ë™ ë°©ì§€
)

# LLM í˜¸ì¶œ
response = llm.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ],
    temperature=0,
)
```

#### 2. Header Inference
```python
from src.prompts.header_inference import (
    format_header_inference_user_prompt
)

# System prompt
system_prompt = HEADER_INFERENCE_SYSTEM_PROMPT

# User prompt í¬ë§·íŒ…
sample_rows = [
    ["WBC", "12.5", "10^9/L", "6.0-17.0"],
    ["RBC", "7.2", "10^12/L", "5.0-10.0"],
]

user_prompt = format_header_inference_user_prompt(sample_rows)

# LLM í˜¸ì¶œ
response = llm.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ],
    temperature=0,
    response_format={"type": "json_object"},
)
```

#### 3. Chat (ìŠ¤ëª°í†¡)
```python
from src.prompts import (
    CHAT_SYSTEM_PROMPT,
    EMERGENCY_SYSTEM_PROMPT,
)

# ì¼ë°˜ ëŒ€í™”
system_prompt = CHAT_SYSTEM_PROMPT
user_input = "ìš°ë¦¬ ê³ ì–‘ì´ê°€ ë°¥ì„ ì•ˆ ë¨¹ì–´ìš”"

response = llm.chat.completions.create(
    model="gpt-5-mini",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input},
    ],
    temperature=0.7,
)

# ì‘ê¸‰ ìƒí™© ê°ì§€ ì‹œ
system_prompt = EMERGENCY_SYSTEM_PROMPT
```

#### 4. Lab Analysis (ê²€ì‚¬ì§€ ë¶„ì„)
```python
from src.prompts.lab_analysis import (
    LAB_ANALYSIS_SYSTEM_PROMPT,
    format_lab_analysis_user_prompt,
)

# System prompt
system_prompt = LAB_ANALYSIS_SYSTEM_PROMPT

# ê²€ì‚¬ ë°ì´í„° (JSON í˜•ì‹)
document_context = """
{
  "hospital_name": "í«í”ŒëŸ¬ìŠ¤ë™ë¬¼ë³‘ì›",
  "patient_name": "ë‚˜ë¹„",
  "inspection_date": "2025-01-20",
  "tests": [
    {"code": "WBC", "value": "12.5", "unit": "10^9/L", "reference_min": "6.0", "reference_max": "17.0"},
    {"code": "RBC", "value": "7.2", "unit": "10^12/L", "reference_min": "5.0", "reference_max": "10.0"}
  ]
}
"""

user_prompt = format_lab_analysis_user_prompt(
    document_context=document_context,
    user_question="ê°„ ìˆ˜ì¹˜ê°€ ê±±ì •ë¼ìš”"  # ì˜µì…˜
)

response = llm.chat.completions.create(
    model="gpt-4.1",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ],
    temperature=0,
)
```

#### 5. Intent Classification (ì˜ë„ ë¶„ë¥˜)
```python
from src.prompts import INTENT_CLASSIFICATION_SYSTEM_PROMPT

# System prompt
system_prompt = INTENT_CLASSIFICATION_SYSTEM_PROMPT

# ì‚¬ìš©ì ì…ë ¥
user_input = "í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼ ì¢€ ë´ì¤˜"

response = llm.chat.completions.create(
    model="gpt-5-nano",  # ë¹ ë¥¸ ê²½ëŸ‰ ëª¨ë¸
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input},
    ],
    temperature=0,
    max_tokens=100,
)

# ì‘ë‹µ: {"intent": "lab_analysis", "confidence": 0.95}
```

## ğŸ”§ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ê°€ì´ë“œ

### í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ì‹œ
1. í•´ë‹¹ ëª¨ë“ˆ íŒŒì¼ ìˆ˜ì • (`metadata_extraction.py` ë˜ëŠ” `header_inference.py`)
2. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìœ¼ë¡œ ê²€ì¦
3. Git commit (í”„ë¡¬í”„íŠ¸ ë³€ê²½ ì´ë ¥ ì¶”ì )

### ìƒˆ í”„ë¡¬í”„íŠ¸ ì¶”ê°€ ì‹œ
1. ìƒˆ ëª¨ë“ˆ íŒŒì¼ ìƒì„± (ì˜ˆ: `quality_check.py`)
2. `__init__.py`ì— export ì¶”ê°€
3. ì‚¬ìš©ì²˜ì—ì„œ import

## ğŸ“‹ í”„ë¡¬í”„íŠ¸ ëª©ë¡

### 1. metadata_extraction.py

#### PATIENT_NAME_SYSTEM_PROMPT
- **ìš©ë„**: ê²€ì‚¬ì§€ í—¤ë”ì—ì„œ patient_name(í™˜ìëª…/ë°˜ë ¤ë™ë¬¼ëª…) ì¶”ì¶œ
- **í•µì‹¬ ê·œì¹™**:
  - client_name(ë³´í˜¸ìëª…)ê³¼ í˜¼ë™ ê¸ˆì§€
  - í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
  - ë¼ë²¨ ì œê±° í›„ ì´ë¦„ë§Œ ë°˜í™˜

#### format_patient_name_user_prompt()
- **ìš©ë„**: patient_name ì¶”ì¶œìš© user í”„ë¡¬í”„íŠ¸ ìƒì„±
- **íŒŒë¼ë¯¸í„°**:
  - `header_text`: í—¤ë” í…ìŠ¤íŠ¸ ë¸”ë¡
  - `client_name`: ì´ë¯¸ ì¶”ì¶œëœ client_name (ì˜µì…˜)

### 2. header_inference.py

#### HEADER_INFERENCE_SYSTEM_PROMPT
- **ìš©ë„**: í…Œì´ë¸” ì»¬ëŸ¼ ì—­í• (name/result/unit/reference/min/max) ì¶”ë¡ 
- **í•µì‹¬ ê·œì¹™**:
  - reference ë˜ëŠ” (min, max) ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©
  - ì¤‘ë³µ col_index ê¸ˆì§€
  - JSON í˜•ì‹ ì‘ë‹µ

#### format_header_inference_user_prompt()
- **ìš©ë„**: í—¤ë” ì¶”ë¡ ìš© user í”„ë¡¬í”„íŠ¸ ìƒì„±
- **íŒŒë¼ë¯¸í„°**:
  - `sample_rows`: í…Œì´ë¸” ë°”ë”” ìƒ˜í”Œ í–‰ ë¦¬ìŠ¤íŠ¸

### 3. chat.py

#### CHAT_SYSTEM_PROMPT
- **ìš©ë„**: ìŠ¤ëª°í†¡/ì¼ë°˜ ëŒ€í™” ì‘ë‹µ ìƒì„±
- **í•µì‹¬ ê·œì¹™**:
  - ì¹œê·¼í•˜ê³  ê³µê°ì ì¸ í†¤
  - ì§ì ‘ ì§„ë‹¨/ì²˜ë°© ê¸ˆì§€
  - ì‘ê¸‰ ìƒí™© ì‹œ ë³‘ì› ë°©ë¬¸ ê¶Œìœ 

#### EMERGENCY_SYSTEM_PROMPT
- **ìš©ë„**: ì‘ê¸‰ ìƒí™© ê°ì§€ ì‹œ ì‚¬ìš©
- **í•µì‹¬ ê·œì¹™**:
  - ì¦‰ì‹œ ë™ë¬¼ë³‘ì› ë°©ë¬¸ ê°•ë ¥ ê¶Œìœ 
  - ì ˆëŒ€ ì§„ë‹¨/ì²˜ë°© ê¸ˆì§€
  - 24ì‹œê°„ ë™ë¬¼ë³‘ì› ì•ˆë‚´

### 4. lab_analysis.py

#### LAB_ANALYSIS_SYSTEM_PROMPT
- **ìš©ë„**: OCR ì¶”ì¶œëœ ê²€ì‚¬ ê²°ê³¼ ë¶„ì„ ë° ì„¤ëª…
- **ì¶œë ¥ í˜•ì‹**:
  1. ë°ì´í„°í”„ë ˆì„ í‘œ (í•­ëª©/ê°’/ë‹¨ìœ„/ì°¸ê³ ë²”ìœ„/ì •ìƒì—¬ë¶€/ë°©í–¥/ì¤‘ì¦ë„)
  2. ì¢…í•© ì„ìƒ íŒë‹¨ê³¼ ì†Œê²¬
- **í•µì‹¬ ê·œì¹™**:
  - ì ˆëŒ€ ì§„ë‹¨/ì²˜ë°© ê¸ˆì§€
  - ì‘ê¸‰ ì§•í›„ ì‹œ ì¦‰ì‹œ ë³‘ì› ê¶Œìœ 
  - ë¶ˆí™•ì‹¤ì„± ëª…ì‹œ
  - "ì°¸ê³ ìš©" ì•ˆë‚´ í•„ìˆ˜

#### format_lab_analysis_user_prompt()
- **ìš©ë„**: ê²€ì‚¬ì§€ ë¶„ì„ìš© user í”„ë¡¬í”„íŠ¸ ìƒì„±
- **íŒŒë¼ë¯¸í„°**:
  - `document_context`: OCR ì¶”ì¶œ ë°ì´í„°
  - `user_question`: ì‚¬ìš©ì ì¶”ê°€ ì§ˆë¬¸ (ì˜µì…˜)

### 5. intent_classification.py

#### INTENT_CLASSIFICATION_SYSTEM_PROMPT
- **ìš©ë„**: ì‚¬ìš©ì ì…ë ¥ì˜ ì˜ë„ë¥¼ 6ê°€ì§€ë¡œ ë¶„ë¥˜
- **ì˜ë„ ìœ í˜•**:
  1. `lab_analysis`: ê²€ì‚¬ì§€/ê²€ì§„ ê²°ê³¼ ë¶„ì„ ìš”ì²­
  2. `health_question`: ê±´ê°• ê´€ë ¨ ì¼ë°˜ ì§ˆë¬¸
  3. `emergency`: ì‘ê¸‰ ìƒí™©
  4. `upload_help`: ì—…ë¡œë“œ ë°©ë²• ë¬¸ì˜
  5. `smalltalk`: ì¼ë°˜ ëŒ€í™”, ì¸ì‚¬, ì¡ë‹´
  6. `other`: ê¸°íƒ€/ë¶„ë¥˜ ë¶ˆê°€
- **ì‘ë‹µ í˜•ì‹**: JSON `{"intent": "ìœ í˜•", "confidence": 0.0-1.0}`

## ğŸ¨ í”„ë¡¬í”„íŠ¸ ì‘ì„± ê°€ì´ë“œë¼ì¸

### 1. ëª…í™•í•œ ì—­í•  ì •ì˜
```python
SYSTEM_PROMPT = """You are an expert at [êµ¬ì²´ì  ì—­í• ].
Given [ì…ë ¥ í˜•ì‹], [ìˆ˜í–‰ ì‘ì—…].
"""
```

### 2. IMPORTANT RULES ì„¹ì…˜
- í˜¼ë™í•˜ê¸° ì‰¬ìš´ ê°œë… ëª…ì‹œ
- ì—£ì§€ ì¼€ì´ìŠ¤ ì²˜ë¦¬ ë°©ë²•
- ì¶œë ¥ í˜•ì‹ ëª…ì‹œ

### 3. ì˜ˆì‹œ í¬í•¨ (í•„ìš” ì‹œ)
```python
SYSTEM_PROMPT = """...
Examples:
- Input: "í™˜ì: ë‚˜ë¹„" â†’ Output: "ë‚˜ë¹„"
- Input: "ë³´í˜¸ì: ê¹€ì² ìˆ˜" â†’ Output: ""
"""
```

### 4. ì¶œë ¥ í˜•ì‹ ëª…ì‹œ
```python
SYSTEM_PROMPT = """...
Output format: plain text | JSON | list
"""
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### í”„ë¡¬í”„íŠ¸ import í…ŒìŠ¤íŠ¸
```bash
poetry run python -c "
from src.services.lab_extraction.prompts import (
    PATIENT_NAME_SYSTEM_PROMPT,
    HEADER_INFERENCE_SYSTEM_PROMPT,
)
print('âœ… Import ì„±ê³µ')
print(f'patient_name í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(PATIENT_NAME_SYSTEM_PROMPT)}')
print(f'header_inference í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(HEADER_INFERENCE_SYSTEM_PROMPT)}')
"
```

### í†µí•© í…ŒìŠ¤íŠ¸
```bash
poetry run pytest tests/test_llm_metadata_fallback.py -v
```

## ğŸ” ë””ë²„ê¹… íŒ

### í”„ë¡¬í”„íŠ¸ ì¶œë ¥ í™•ì¸
```python
from src.services.lab_extraction.prompts.metadata_extraction import (
    format_patient_name_user_prompt
)

user_prompt = format_patient_name_user_prompt(
    header_text="í«í”ŒëŸ¬ìŠ¤ë™ë¬¼ë³‘ì›\në³´í˜¸ì: ê¹€ì² ìˆ˜\në‚˜ë¹„",
    client_name="ê¹€ì² ìˆ˜"
)

print(user_prompt)
```

### LLM ì‘ë‹µ í™•ì¸
```python
# lab_table_extractor.pyì˜ _extract_patient_name_with_llm ë©”ì„œë“œì—ì„œ
# LLM ì‘ë‹µì„ ë¡œê¹…í•˜ì—¬ í”„ë¡¬í”„íŠ¸ íš¨ê³¼ ê²€ì¦
```

## ğŸ“š ì°¸ê³  ìë£Œ

- OpenAI Best Practices: https://platform.openai.com/docs/guides/prompt-engineering
- Anthropic Prompt Engineering: https://docs.anthropic.com/claude/docs/prompt-engineering

